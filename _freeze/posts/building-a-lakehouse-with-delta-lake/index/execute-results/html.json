{
  "hash": "74cb3c92de245e8bfe65248a79d54d4b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Building a Lakehouse with Delta Lake\ndescription: 'How to use the Delta Lake python library to manage massive amounts of data at scale.'\nauthor: Dhruv Dole\nkeywords:\n  - python\n  - deltalake\n  - lakehouse\n  - pyarrow\n  - polars\n  - minio\n---\n\n\nThe first step in creating any kind of data processes is setting up a system for efficient storage and querying of your data. There are two major storage concepts for OLAP data storage: Data warehouses and data lakes. Warehouses act as a central source of truth for analytics and reporting processes. Warehouses are designed to efficiently store and query highly structured data. Warehouses are often OLAP databases like ClickHouse or managed offerings like Amazon Redshift, Google BigQuery, or Snowflake.\n\nData Lakes are simply a central storage system for massive amounts of data in any form. This could mean log dumps, json files, CSVs, images... It is also possible to combine the two concepts into the Data Lakehouse. The principle here is storing data using one of the many open table formats(IceBerg, Hudi, Deltalake) in some kind of mass storage, generally cloud object storage like Amazon S3 or Azure Blob Storage. This allows raw, unstructured data to be stored in the same system as structured tabular data. This can provide large cost savings and simplify operations significantly.\n\nThis notebook will explain how I set up my own personal data lakehouse with Minio and Delta Lake, while performing queries with pyArrow and Polars\n\n##  What is A Delta Lake Table\nA Delta Lake table is simply a collection of parquet files with matching schemas, and a json transaction log. I won't go into too much detail here because it is better explained [here](https://delta-io.github.io/delta-rs/how-delta-lake-works/architecture-of-delta-table/).\n\nIt is important to remember that unlike a basic parquet file, delta tables can become [polluted](https://delta-io.github.io/delta-rs/usage/managing-tables/) with 'old' files. I will explain below how to manage this in python.\n\n\n\n\n\n\n## Prerequisites\n\n1. A Minio Bucket with object locking enabled\n2. Minio credentials with R/W access to the bucket\n3. Some tabular data\n\nI will be using a [COVID-19 Open-Dataset](https://github.com/GoogleCloudPlatform/covid-19-open-data).\n\n## Code\n\n### Load Environment Variables\n\n::: {#3fd631d1 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nfrom dotenv import find_dotenv, load_dotenv\n\ntry:\n    env_file = find_dotenv(raise_error_if_not_found=True)\n    load_dotenv(env_file)\nexcept IOError as e:\n    print(e)\n\nMINIO_ENDPOINT = os.environ['MINIO_ENDPOINT']\nMINIO_ACCESS_KEY = os.environ['MINIO_ACCESS_KEY']\nMINIO_SECRET_KEY = os.environ['MINIO_SECRET_KEY']\n```\n:::\n\n\n### Download Data\n\n::: {#02554ce6 .cell execution_count=2}\n``` {.python .cell-code}\nimport polars as pl\ndata_url = 'https://storage.googleapis.com/covid19-open-data/v3/epidemiology.csv'\n\nschema = {\n    'date': pl.Date,\n    'location_key': pl.String,\n    'new_confirmed': pl.Int64,\n    'new_deceased': pl.Int64,\n    'new_recovered': pl.Int64,\n    'new_tested': pl.Int64,\n    'cumulative_confirmed': pl.Int64,\n    'cumulative_deceased': pl.Int64,\n    'cumulative_recovered': pl.Int64,\n    'cumulative_tested': pl.Int64,\n}\ndf = pl.read_csv(data_url, schema=schema)\n\ndf.sample(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>location_key</th><th>new_confirmed</th><th>new_deceased</th><th>new_recovered</th><th>new_tested</th><th>cumulative_confirmed</th><th>cumulative_deceased</th><th>cumulative_recovered</th><th>cumulative_tested</th></tr><tr><td>date</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>2021-09-02</td><td>&quot;BR_PE_260750&quot;</td><td>-7</td><td>0</td><td>null</td><td>null</td><td>998</td><td>13</td><td>null</td><td>null</td></tr><tr><td>2021-02-15</td><td>&quot;US_MO_29015&quot;</td><td>1</td><td>0</td><td>null</td><td>null</td><td>1432</td><td>23</td><td>null</td><td>null</td></tr><tr><td>2021-07-01</td><td>&quot;IL_M_0577&quot;</td><td>0</td><td>0</td><td>0</td><td>null</td><td>671</td><td>1</td><td>661</td><td>null</td></tr><tr><td>2020-12-16</td><td>&quot;BR_MG_311030&quot;</td><td>5</td><td>0</td><td>0</td><td>8</td><td>219</td><td>5</td><td>0</td><td>674</td></tr><tr><td>2020-09-16</td><td>&quot;RU_YAR&quot;</td><td>53</td><td>2</td><td>69</td><td>null</td><td>7904</td><td>45</td><td>7419</td><td>null</td></tr><tr><td>2020-08-18</td><td>&quot;IN_CT_MHS&quot;</td><td>2</td><td>1</td><td>17</td><td>0</td><td>282</td><td>5</td><td>188</td><td>2183</td></tr><tr><td>2021-07-02</td><td>&quot;PL_20_13&quot;</td><td>0</td><td>0</td><td>null</td><td>60</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2021-12-31</td><td>&quot;US_MA_25001&quot;</td><td>260</td><td>3</td><td>null</td><td>null</td><td>24650</td><td>559</td><td>null</td><td>null</td></tr><tr><td>2022-03-11</td><td>&quot;BR_AL_270510&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>1675</td><td>45</td><td>null</td><td>null</td></tr><tr><td>2020-10-08</td><td>&quot;DE_SH_01004&quot;</td><td>1</td><td>0</td><td>1</td><td>null</td><td>172</td><td>3</td><td>169</td><td>null</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### Create Empty Deltalake Schema\n::: { .callout-note }\nThe variable `dtable_schema_name` maps to a key prefix added after the `bucket_name` and before the actual Delta Table files. This means we can store as many different tables in one bucket as we want.\n:::\n\n::: {#dcd99c91 .cell execution_count=3}\n``` {.python .cell-code}\nfrom deltalake import DeltaTable\n\n# Minio Connection Parameters\nstorage_options = {\n    'endpoint_url': MINIO_ENDPOINT,\n    'AWS_ACCESS_KEY_ID': MINIO_ACCESS_KEY,\n    'AWS_SECRET_ACCESS_KEY': MINIO_SECRET_KEY,\n    'conditional_put': 'etag' #https://delta-io.github.io/delta-rs/usage/writing/writing-to-s3-with-locking-provider/#enabling-concurrent-writes-for-alternative-clients\n}\nbucket_name = 'deltalake-demo'\ndtable_schema_name = 'covid'\n\ndtable_schema = df.to_arrow().schema  # convert dataframe schema to pyArrow\n\ndtable = DeltaTable.create(table_uri=f's3a://{bucket_name}/{dtable_schema_name}', schema=dtable_schema,\n                           storage_options=storage_options)\n```\n:::\n\n\n### Write Dataframe to Delta Lake\n\n::: {#4344ad34 .cell execution_count=4}\n``` {.python .cell-code}\ndf.write_delta(dtable, mode='append')\n```\n:::\n\n\n### Querying the Delta Lake\nNow that our dataset is in the DeltaLake we have to be able to query it without loading all of it into memory at once. We can use Polars or PyArrow for this purpose.\n\n#### Polars\n\n::: {#56b90aac .cell execution_count=5}\n``` {.python .cell-code}\n# Create a LazyFrame representing the Delta Table\nldf = pl.scan_delta(dtable, use_pyarrow=True)\nldf.collect_schema()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nSchema([('date', Date),\n        ('location_key', String),\n        ('new_confirmed', Int64),\n        ('new_deceased', Int64),\n        ('new_recovered', Int64),\n        ('new_tested', Int64),\n        ('cumulative_confirmed', Int64),\n        ('cumulative_deceased', Int64),\n        ('cumulative_recovered', Int64),\n        ('cumulative_tested', Int64)])\n```\n:::\n:::\n\n\nReturn the first 10 records:\n\n::: {#00c3391b .cell execution_count=6}\n``` {.python .cell-code}\nldf.head(10).collect()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>location_key</th><th>new_confirmed</th><th>new_deceased</th><th>new_recovered</th><th>new_tested</th><th>cumulative_confirmed</th><th>cumulative_deceased</th><th>cumulative_recovered</th><th>cumulative_tested</th></tr><tr><td>date</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>2020-01-01</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr><tr><td>2020-01-02</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr><tr><td>2020-01-03</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr><tr><td>2020-01-04</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr><tr><td>2020-01-05</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr><tr><td>2020-01-06</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr><tr><td>2020-01-07</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr><tr><td>2020-01-08</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr><tr><td>2020-01-09</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr><tr><td>2020-01-10</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nFind all records in July 2022 and return the first 10:\n\n::: {#79f91372 .cell execution_count=7}\n``` {.python .cell-code}\nldf.filter(\n    (pl.col('date') >= pl.date(2022, 7, 1)) &\n    (pl.col('date') < pl.date(2022, 8, 1))\n).collect().head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>location_key</th><th>new_confirmed</th><th>new_deceased</th><th>new_recovered</th><th>new_tested</th><th>cumulative_confirmed</th><th>cumulative_deceased</th><th>cumulative_recovered</th><th>cumulative_tested</th></tr><tr><td>date</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>2022-07-01</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>44177</td><td>153</td><td>null</td><td>null</td></tr><tr><td>2022-07-02</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>44177</td><td>153</td><td>null</td><td>null</td></tr><tr><td>2022-07-03</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>44177</td><td>153</td><td>null</td><td>null</td></tr><tr><td>2022-07-04</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>44177</td><td>153</td><td>null</td><td>null</td></tr><tr><td>2022-07-05</td><td>&quot;AD&quot;</td><td>494</td><td>0</td><td>null</td><td>null</td><td>44671</td><td>153</td><td>null</td><td>null</td></tr><tr><td>2022-07-06</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>44671</td><td>153</td><td>null</td><td>null</td></tr><tr><td>2022-07-07</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>44671</td><td>153</td><td>null</td><td>null</td></tr><tr><td>2022-07-08</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>44671</td><td>153</td><td>null</td><td>null</td></tr><tr><td>2022-07-09</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>44671</td><td>153</td><td>null</td><td>null</td></tr><tr><td>2022-07-10</td><td>&quot;AD&quot;</td><td>0</td><td>0</td><td>null</td><td>null</td><td>44671</td><td>153</td><td>null</td><td>null</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nFind all records in July 2022 and find the average `new_confirmed` cases per `location_key`:\n\n::: {#8145f01e .cell execution_count=8}\n``` {.python .cell-code}\nldf.filter(\n    (pl.col('date') >= pl.date(2022, 7, 1)) &\n    (pl.col('date') < pl.date(2022, 8, 1))\n).group_by('location_key').agg(pl.col('new_confirmed').mean()).collect().head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>location_key</th><th>new_confirmed</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;BR_GO_520500&quot;</td><td>0.741935</td></tr><tr><td>&quot;BR_MG_314655&quot;</td><td>1.483871</td></tr><tr><td>&quot;IL_Z_0874&quot;</td><td>27.903226</td></tr><tr><td>&quot;BR_PA_150172&quot;</td><td>14.806452</td></tr><tr><td>&quot;BR_RR_140005&quot;</td><td>10.935484</td></tr><tr><td>&quot;ES_CT_43161&quot;</td><td>3.761905</td></tr><tr><td>&quot;BR_RN_240420&quot;</td><td>2.548387</td></tr><tr><td>&quot;BR_SE_280700&quot;</td><td>0.129032</td></tr><tr><td>&quot;BR_MG_312010&quot;</td><td>0.0</td></tr><tr><td>&quot;IL_Z_4203&quot;</td><td>0.806452</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n#### PyArrow\n::: { .callout-note }\nWhile PyArrow doesn't have many of the analytical features of Polars, It excels at transforming massive amounts of data between different storage formats. For example, I use PyArrow to transfer data between DeltaLake tables and ClickHouse databases).\n:::\n\n::: {#9b9b3334 .cell execution_count=9}\n``` {.python .cell-code}\n# Instantiate the PyArrow Dataset\nds = dtable.to_pyarrow_dataset()\nds.schema\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\ndate: date32[day]\nlocation_key: string\nnew_confirmed: int64\nnew_deceased: int64\nnew_recovered: int64\nnew_tested: int64\ncumulative_confirmed: int64\ncumulative_deceased: int64\ncumulative_recovered: int64\ncumulative_tested: int64\n```\n:::\n:::\n\n\nFind all records in July 2022 and find the average `new_confirmed` cases per `location_key`:\n\n::: {#408e9c97 .cell execution_count=10}\n``` {.python .cell-code}\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom datetime import date\n\njuly_table = ds.filter(\n    ((pc.field('date') >= pa.scalar(date(2022, 7, 1), type=pa.date32())) &\n     (pc.field('date') < pa.scalar(date(2022, 8, 1), type=pa.date32()))\n     )\n).to_table()\n\nresult = pa.TableGroupBy(july_table, 'location_key').aggregate([('new_confirmed', 'mean')])\nresult\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\npyarrow.Table\nlocation_key: string\nnew_confirmed_mean: double\n----\nlocation_key: [[\"BR_MG_314875\",\"BR_MG_314880\",\"BR_MG_314890\",\"BR_MG_314900\",\"BR_MG_314910\",...,\"IL_WE_3779\",\"IL_WE_3780\",\"IL_WE_3788\",\"IN_UT_BGS\",\"BR_MT_510517\"]]\nnew_confirmed_mean: [[0.4838709677419355,2.064516129032258,3.5483870967741935,0,1.7741935483870968,...,1.2258064516129032,3.903225806451613,0.6774193548387096,0.41935483870967744,12.290322580645162]]\n```\n:::\n:::\n\n\nPyArrow has no integrated way to view data so we have use `result.to_pandas()` or convert the table into a polars df:\n\n::: {#e8cbd02f .cell execution_count=11}\n``` {.python .cell-code}\npl.DataFrame(result).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>location_key</th><th>new_confirmed_mean</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;BR_MG_314875&quot;</td><td>0.483871</td></tr><tr><td>&quot;BR_MG_314880&quot;</td><td>2.064516</td></tr><tr><td>&quot;BR_MG_314890&quot;</td><td>3.548387</td></tr><tr><td>&quot;BR_MG_314900&quot;</td><td>0.0</td></tr><tr><td>&quot;BR_MG_314910&quot;</td><td>1.774194</td></tr><tr><td>&quot;BR_MG_314915&quot;</td><td>0.0</td></tr><tr><td>&quot;BR_MG_314920&quot;</td><td>0.387097</td></tr><tr><td>&quot;BR_MG_314930&quot;</td><td>0.0</td></tr><tr><td>&quot;BR_MG_314940&quot;</td><td>0.0</td></tr><tr><td>&quot;BR_MG_314950&quot;</td><td>0.0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### DeltaLake Maintenance\nAs I mentioned earlier, delta tables will become polluted over time. This can be managed with the following:\n\nCompacting the table is useful for turning a table made up of many small files into fewer larger ones.\n\n::: {#0b5ad40f .cell execution_count=12}\n``` {.python .cell-code}\ndtable.optimize.compact()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n{'numFilesAdded': 1,\n 'numFilesRemoved': 20,\n 'filesAdded': '{\"avg\":58140236.0,\"max\":58140236,\"min\":58140236,\"totalFiles\":1,\"totalSize\":58140236}',\n 'filesRemoved': '{\"avg\":3854084.2,\"max\":4648234,\"min\":3193592,\"totalFiles\":20,\"totalSize\":77081684}',\n 'partitionsOptimized': 1,\n 'numBatches': 12244,\n 'totalConsideredFiles': 20,\n 'totalFilesSkipped': 0,\n 'preserveInsertionOrder': True}\n```\n:::\n:::\n\n\nThe vacuum command is a garbage collector which cleans files which have been marked for deletion. More on this [here](https://delta-io.github.io/delta-rs/usage/managing-tables/).\n\n::: {#779a2336 .cell execution_count=13}\n``` {.python .cell-code}\ndtable.vacuum() # Does nothing in this instance because we haven't deleted any rows.\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n[]\n```\n:::\n:::\n\n\n## Conclusion\nDelta Lake is a powerful tool for managing massive amounts of data efficiently and cost-effectively. It provides robust features like ACID transactions, scalable metadata handling, and efficient data storage formats, ensuring data integrity and reliability. By leveraging Delta Lake, organizations can perform complex data operations without compromizing on cost or performance.\n\n\n### Resources\n- [Delta Lake Python Docs](https://delta-io.github.io/delta-rs/)\n- [Polars User Guide](https://docs.pola.rs/)\n- [Polars - DeltaLake integration](https://delta-io.github.io/delta-rs/integrations/delta-lake-polars/)\n- [PyArrow Docs](https://arrow.apache.org/docs/python/index.html)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}